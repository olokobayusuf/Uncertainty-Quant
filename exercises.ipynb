{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.6.5-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python36564bit19053807de3b44f894d4c8758c328f67",
   "display_name": "Python 3.6.5 64-bit"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Bernoulli Distribution\n",
    "1. What is the sample space of the Bernoulli distribution?\n",
    "\n",
    "$\\Omega = \\{ 0, 1 \\}$\n",
    "\n",
    "2. What is is corresponding probability measure?\n",
    "\n",
    "$\\mu(x) = \\frac{1}{2}$."
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Transformations on Random Variables\n",
    "\n",
    "3. Suppose $X$ is uniform on $[0, 2\\pi]$. Find the density of $Y = \\sin X$.\n",
    "\n",
    "4. Let $X_1$ and $X_2$ be two independent uniform distributions on $[0, 1]$.\n",
    "    - Find the density of $Y_1 = X_1 + X_2$\n",
    "\n",
    "    - Find the density of $Y_2 = X_1 - X_2$\n",
    "\n",
    "    - Find the density of $Y_3 = X_1 / X_2$\n",
    "\n",
    "    - Find the density of $Y_4 = \\max(X_1, X_2)$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Expected Values and Moments\n",
    "\n",
    "5. Find the mean and variance of a Gaussian random variable $X$ with a density:\n",
    "$$ p(x) = \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} \\exp{ \\left(- \\frac{(x - m)^2}{2 \\sigma^2 } \\right)} $$\n",
    "\n",
    "6. Find the mean of the Cauchy distribution:\n",
    "$$ p(x) = \\frac{1}{\\pi (1 + x^2)} $$\n",
    "\n",
    "7. Find the mean and variance of the Binomial distribution:\n",
    "$$ b(x; n, p) = \\begin{pmatrix} n \\\\ x \\end{pmatrix} p^x (1-p)^{n - x} $$\n",
    "\n",
    "$$ \\mathbb{E}[X], X \\sim b(x; n, p) = \\sum_{x=0}^n x \\begin{pmatrix} n \\\\ x \\end{pmatrix} p^x (1-p)^{n - x} $$\n",
    "$$ = \\sum_{x=0}^n x \\frac{n!}{x! (n-x)!} p^x (1-p)^{n - x} $$\n",
    "$$ = 0 + \\sum_{x=1}^n \\frac{n!}{(x-1)! (n-x)!} p^x (1-p)^{n - x} $$\n",
    "\n",
    "Let $y = x - 1$ and $m = n - 1$, so that $x = y + 1$ and $n = m + 1$:\n",
    "    \n",
    "$$ = \\sum_{y=0}^m \\frac{(m+1)!}{y! (m - y)!} p^{y+1} (1 - p)^{m - y} $$\n",
    "$$ = (m + 1) p \\sum_{y=0}^m \\frac{m!}{y! (m - y)!} p^y (1 - p)^{m - y} $$\n",
    "$$ = n p \\sum_{y=0}^m \\frac{m!}{y! (m - y)!} p^y (1 - p)^{m - y} $$\n",
    "$$ = np \\sum_{y=0}^m \\begin{pmatrix} m \\\\ y \\end{pmatrix} p^y (1 - p)^{m - y} $$\n",
    "\n",
    "But by the binomial theorem, we have:\n",
    "$$ = np (p + (1 - p))^m $$\n",
    "$$ \\mathbb{E}[X] = np $$\n",
    "\n",
    "We compute variance as:\n",
    "$$ Var(X) = \\mathbb{E}[X^2] - \\mathbb{E}[X]^2 $$\n",
    "$$ \\mathbb{E}[X^2] = \\sum_{x = 0}^n x^2 \\begin{pmatrix} n \\\\ x \\end{pmatrix} p^x (1-p)^{n - x} $$\n",
    "$$ = \\sum_{x = 0}^n x^2 \\frac{n!}{x! (n - x)!} p^x (1-p)^{n - x} $$\n",
    "$$ = \\sum_{x = 0}^n nx \\frac{n-1!}{(x-1)! (n - x)!} p^x (1-p)^{n - x} $$\n",
    "$$ = \\sum_{x = 0}^n nx \\begin{pmatrix} n - 1 \\\\ x - 1 \\end{pmatrix} p^x (1-p)^{n - x} $$\n",
    "$$ = 0 + \\sum_{x = 1}^n nx \\begin{pmatrix} n - 1 \\\\ x - 1 \\end{pmatrix} p^x (1-p)^{n - x} $$\n",
    "$$ = np \\sum_{x = 1}^n x \\begin{pmatrix} n - 1 \\\\ x - 1 \\end{pmatrix} p^{x-1} (1-p)^{n - x} $$\n",
    "\n",
    "Let $y = x - 1$ and $m = n - 1$, so that $x = y + 1$ and $n = m + 1$:\n",
    "$$ = np \\sum_{y=0}^m (y+1) \\begin{pmatrix} m \\\\ y \\end{pmatrix} p^{y} (1 - p)^{m - y} $$\n",
    "$$ = np \\left( \\sum_{y=0}^m y \\begin{pmatrix} m \\\\ y \\end{pmatrix} p^{y} (1 - p)^{m - y} + \\sum_{y=0}^m \\begin{pmatrix} m \\\\ y \\end{pmatrix} p^{y} (1 - p)^{m - y} \\right) $$\n",
    "$$ = np \\left( \\sum_{y=0}^m y \\begin{pmatrix} m \\\\ y \\end{pmatrix} p^{y} (1 - p)^{m - y} + 1 \\right)$$\n",
    "\n",
    "The first term is the expected value of the binomial distribution with respect to $y$ and $m$, hence we can recursively apply the proof from $\\mathbb{E}[X]$:\n",
    "$$ = np ( mp + 1 ) $$\n",
    "$$ = np ((n - 1)p + 1)$$\n",
    "$$ = np (np - p + 1) $$\n",
    "$$ \\mathbb{E}[X^2] = (np)^2 + np(1 - p) $$\n",
    "\n",
    "Therefore:\n",
    "$$ Var(X) = \\mathbb{E}[X^2] - \\mathbb{E}[X]^2 $$\n",
    "$$ = (np)^2 + np(1 - p) - (np)^2 $$\n",
    "$$ = np(1 - p) $$\n",
    "\n",
    "8. Let $X$ be a random variable such that $\\mathbb{E}[|X|^m] \\leq AC^m$ for some positive constants $A$ and $C$, and all integers $m \\geq 0$. Show that $\\mu (|X| > C) = 0$."
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Joint Probability and Independence\n",
    "9. Generate a sample of two random variables $X$ and $Y$ where $X$ and $Y$ are normal with a correlation $\\rho$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Correlation: [[1.         0.21478601]\n [0.21478601 1.        ]]\n"
    }
   ],
   "source": [
    "from numpy import array, corrcoef, stack\n",
    "from numpy.linalg import cholesky\n",
    "from numpy.random import normal\n",
    "\n",
    "N = 1000\n",
    "COR = 0.2\n",
    "# Generate samples\n",
    "samples = normal(loc=0., scale=1., size=(N, 2))\n",
    "transformation = array([\n",
    "    [1., COR],\n",
    "    [COR, 1.]\n",
    "])\n",
    "samples = samples @ cholesky(transformation) # Square root of transformation\n",
    "# Compute covariance\n",
    "rho = corrcoef(samples.T)\n",
    "print(f\"Correlation: {rho}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Conditional Probability and Conditional Expectation\n",
    "10. Let $X$ and $Y$ be two random variables with $\\mathbb{E}[Y] = m$ and $\\mathbb{E}[Y^2] < \\infty$.\n",
    "- Show that the constant $c$ that minimizes $\\mathbb{E}[(Y - c)^2]$ is $c  = m$.\n",
    "\n",
    "$$ \\mathbb{E}[(Y - c)^2] = \\mathbb{E}[Y^2 - 2cY + c^2] $$\n",
    "$$ = \\mathbb{E}[Y] - 2c\\mathbb{E}[Y] + c^2 $$\n",
    "$$ = c^2 - 2cm + m $$\n",
    "$$ \\frac{\\delta}{\\delta c} = 2c - 2m $$\n",
    "$$ \\therefore c^* = m $$\n",
    "\n",
    "- Show that the random variable $f(X)$ that minimizes $\\mathbb{E}[(Y - f(X))^2\\ \\vert\\ X]$ is $f(X) = E[Y\\ \\vert\\ X]$.\n",
    "\n",
    "$$ \\mathbb{E}[(Y - f(x))^2] = \\mathbb{E}[Y^2 - 2f(x)Y + f^2(x)] $$\n",
    "$$ =\\mathbb{E}[Y^2 | X] - 2f(x) \\mathbb{E}[Y | X] + f^2(x) $$\n",
    "$$ \\frac{\\delta f(x)}{\\delta x} = -2\\mathbb{E}[Y | X] + 2f(x) $$\n",
    "$$ \\therefore f^*(x) = E[Y | X] $$\n",
    "\n",
    "- Show that the random variable $f(X)$ that minimizes $\\mathbb{E}[(Y - f(X))^2]$ is also $f(X) = E[Y\\ \\vert\\ X]$.\n",
    "\n",
    "11. Will you consider a coin asymmetric if after 1000 tosses, the number of heads equals 600?"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Maximum Likelihood Estimation\n",
    "12. Let $\\{ X_1, \\ldots, X_n \\}$ be i.i.d samples from $\\mathcal{U}(0., \\theta)$. Find $\\hat{\\theta}$ that maximizes the MLE.\n",
    "\n",
    "13. Show that the mean of $\\frac{\\delta}{\\delta x} \\ln p(x; \\theta) = 0$\n",
    "\n",
    "14. Show that the mean of $\\frac{\\delta^2}{\\delta x^2} \\ln p(x; \\theta) = Var(\\frac{\\delta}{\\delta x} \\ln p(x; \\theta)) = I(\\theta)$"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Non-Parametric Inference\n",
    "15. Show that for some empirical distribution, $\\mathbb{E}[\\hat{P}_n(x)] = P(x)$ and $Var(\\hat{P}_n(x)) = \\frac{P(x)(1-P(x))}{n}$\n",
    "\n",
    "16. Derive the Nadaraya-Watson non-parametric regression technique."
   ]
  }
 ]
}